{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Interpretation\n",
    "<br> <br> \n",
    "#### - Confidence Based on Tree Variance\n",
    "> The variance of the predictions of the trees. The prediction is usually the average of input values of the dependent variables. This confidence calcualte the variance regarding this final prediction (overall average).\n",
    "\n",
    "<br> \n",
    "Calcualte variance in a group of samples/observations to check the variance of the prediction. If the variance is high, we may not be confident of the prediction result.\n",
    "<br> <br> \n",
    "<br> <br> <br> \n",
    "#### - Feature Importance\n",
    "> To check which features have great importance on the prediction result. This is a good way for analysis, remove some columns or add weights to improve the prediction result.\n",
    "\n",
    "\n",
    "<br> \n",
    "Steps:\n",
    "* Separate the dataset into independent variables `X_trian` and dependent variables `y_trian` for training.\n",
    "* Use the training data to build a random forest model.\n",
    "* Compare the prediction result `y_pred` with `y_train` and calcualte `score`.\n",
    "* Want to find which features (columns in `X_train`) influence prediction most without rebuilding the model.\n",
    "* Shuffel one column and feed the new dataset to the model again to get prediction result `y_sub`.\n",
    "* Compare the prediction result `y_sub` with `y_train` and calcualte `score_sub`.\n",
    "* Compare `score_sub` with `score`.\n",
    "* Rank `score_sub`s to check which features have great effect on the overall prediction.\n",
    "\n",
    "<br> <br> <br> \n",
    "#### - Partial Dependence\n",
    "> Analysis an individual fator that influences the overall prediction result assuming that all other features are equable. Used to specify the relationship between one fator and the prediction. For example, how did `YearMade` influences the dependent variable `SalePrice` over the past 50 years. \n",
    "<br> <br> Pull out the underlying truth of feature importance.\n",
    "\n",
    "\n",
    "<br> \n",
    "Steps:\n",
    "* Leave other factors constant (just what they are) in the dataset.\n",
    "* Do partial dependence plot regarding each feature by set it a __constant value__ each time instead of shuffling the colum (feature importance).\n",
    "<br><br>\n",
    "[Do the rest same as calculating feature importance]<br><br>\n",
    "* Plot the chart __for each sample__ (row in the dataset) as the __constant value__ changes.\n",
    "* Calcualte the median value across lines of all samples.\n",
    "* Do cluster analysis to find out a few different shapes (one shape for one cluster).\n",
    "\n",
    "<br> <br> <br> \n",
    "#### - Tree Interpreter\n",
    "> One obervation (one row in the dataset can only have __one path__ through out the tree). Check how each decision or feature influences the final prediction result __for a particular observation.__<br><br>\n",
    "Pick out __one particular__ row/observation.<br> <br>\n",
    "Get the `prediction` of that observation and the overall `bias` for all data.<br> <br> \n",
    "Get the `contribution` measuring a set of feature importance for this observation.\n",
    "\n",
    "<br> \n",
    "Steps:\n",
    "* Track backwards after reaching the leaf node for each observation.\n",
    "* Check how much each decision node has changed the prediction value for the start. \n",
    "<br> (usually record in a table)\n",
    "\n",
    "<br> \n",
    "__Interaction feature importance:__ <br>\n",
    "Although each decision can be analyzied individually, the result at each decision node is the result of the combination or interaction of the current decision and previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *\n",
    "from fastai.tabular import *\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display\n",
    "from sklearn import metrics\n",
    "\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from the old version of fastai (fastai.structured) \n",
    "# https://github.com/fastai/fastai/blob/master/old/fastai/structured.py\n",
    "\n",
    "def numericalize(df, col, name, max_n_cat):\n",
    "    ''' For values not numeric, convert it to corresponding categorical values + 1.abc\n",
    "    e.g. NaN values are not -1 but 0.'''\n",
    "    \n",
    "    if not is_numeric_dtype(col) and ( max_n_cat is None or len(col.cat.categories)>max_n_cat):\n",
    "        df[name] = pd.Categorical(col).codes+1\n",
    "    \n",
    "    \n",
    "def fix_missing(df, col, name, na_dict):\n",
    "    if is_numeric_dtype(col):\n",
    "        if pd.isnull(col).sum() or (name in na_dict):\n",
    "            df[name+'_na'] = pd.isnull(col)\n",
    "            filler = na_dict[name] if name in na_dict else col.median()\n",
    "            df[name] = col.fillna(filler)\n",
    "            na_dict[name] = filler\n",
    "    return na_dict\n",
    "\n",
    "\n",
    "def proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,\n",
    "            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n",
    "    if not ignore_flds: ignore_flds=[]\n",
    "    if not skip_flds: skip_flds=[]\n",
    "    if subset: df = get_sample(df,subset)\n",
    "    else: df = df.copy()\n",
    "    ignored_flds = df.loc[:, ignore_flds]\n",
    "    df.drop(ignore_flds, axis=1, inplace=True)\n",
    "    if preproc_fn: preproc_fn(df)\n",
    "    if y_fld is None: y = None\n",
    "    else:\n",
    "        if not is_numeric_dtype(df[y_fld]): df[y_fld] = pd.Categorical(df[y_fld]).codes\n",
    "        y = df[y_fld].values\n",
    "        skip_flds += [y_fld]\n",
    "    df.drop(skip_flds, axis=1, inplace=True)\n",
    "\n",
    "    if na_dict is None: na_dict = {}\n",
    "    else: na_dict = na_dict.copy()\n",
    "    na_dict_initial = na_dict.copy()\n",
    "    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n",
    "    if len(na_dict_initial.keys()) > 0:\n",
    "        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)\n",
    "    if do_scale: mapper = scale_vars(df, mapper)\n",
    "    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n",
    "    df = pd.get_dummies(df, dummy_na=True)\n",
    "    df = pd.concat([ignored_flds, df], axis=1)\n",
    "    res = [df, y, na_dict]\n",
    "    if do_scale: res = res + [mapper]\n",
    "    return res\n",
    "\n",
    "\n",
    "def set_rf_samples(n):\n",
    "    forest._generate_sample_indices = (lambda rs, n_samples:\n",
    "    forest.check_random_state(rs).randint(0, n_samples, n))\n",
    "\n",
    "    \n",
    "def reset_rf_samples():\n",
    "    \"\"\" Undoes the changes produced by set_rf_samples.\n",
    "    \"\"\"\n",
    "    forest._generate_sample_indices = (lambda rs, n_samples:\n",
    "    forest.check_random_state(rs).randint(0, n_samples, n_samples))\n",
    "    \n",
    "    \n",
    "def draw_tree(t, df, size=10, ratio=0.6, precision=0):\n",
    "    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n",
    "                      special_characters=True, rotate=True, precision=precision)\n",
    "    display(graphviz.Source(re.sub('Tree {',\n",
    "       f'Tree {{ size={size}; ratio={ratio}', s)))\n",
    "    \n",
    "    \n",
    "def train_cats(df):\n",
    "    \"\"\"Change any columns of strings in a panda's dataframe to a column of\n",
    "    categorical values. This applies the changes inplace.\n",
    "    \"\"\"\n",
    "    for n,c in df.items():\n",
    "        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()\n",
    "            \n",
    "def split_vals(a, n):\n",
    "    return a[:n], a[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Load in Data from Last Lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/bulldozers/'\n",
    "\n",
    "df_raw = pd.read_feather('tmp/raw')\n",
    "df_trn, y_trn, _ = proc_df(df_raw, 'SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_valid = 12000\n",
    "n_trn = len(df_trn) - n_valid\n",
    "\n",
    "X_train, X_valid = split_vals(df_trn, n_trn)\n",
    "y_train, y_valid = split_vals(y_trn, n_trn)\n",
    "\n",
    "raw_train, raw_valid = split_vals(df_raw, n_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sub = X_train[['YearMade', 'MachineHoursCurrentMeter']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Basic Data Structures\n",
    "<br><br>\n",
    "**np.random.seed(42):** define a seed that makes the random numbers predictable;\n",
    "        generate a random number (the same number until reseed) that starting from 42.\n",
    "<br><br>\n",
    "**np.random.permutation(num):** return back a randomly shuffled int suquence from zero to int *num*(not included). *num* can be int or array_like.\n",
    "<br>\n",
    "                    Can be used in machine learning to generate random samples.\n",
    "<br><br>\n",
    "The **iloc** indexer for Pandas Dataframe is used for integer-location based indexing / selection by position. # Rows: data.iloc[0] # first row of data frame (Aleshia Tomkiewicz) - Note a Series data type output. Multiple columns and rows can be selected together using the .iloc indexer.\n",
    "\n",
    "<br><br>\n",
    "Python property: http://funhacks.net/explore-python/Class/property.html\n",
    "\n",
    "<br><br>\n",
    "Python numpy.arange(): return evenly spaced values within a given interval (default step=1).\n",
    "<br>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.arange.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeEnsemble():\n",
    "    def __init__(self, x, y, n_trees, sample_sz, min_leaf=5):\n",
    "        np.random.seed(42)\n",
    "        self.x, self.y, self.sample_sz, self.min_leaf = x, y, sample_sz, min_leaf\n",
    "        self.trees = [self.create_tree() for i in range(n_trees)]\n",
    "        \n",
    "    def create_tree(self):\n",
    "        rnd_idxs = np.random.permutation(len(self.y))[:self.sample_sz]\n",
    "        return DecisionTree(self.x.iloc[rnd_idxs], self.y[rnd_idxs], min_leaf=self.min_leaf)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.mean([t.predict(x) for t in self.trees], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self.n: the number of rows given to the tree (len(idx) = len(y))\n",
    "<br><br>\n",
    "self.c: the number of columns given to the tree (equals to the number of columns in the independent variables, the second parameter of a dataframe)\n",
    "<br><br>\n",
    "self.val: for this tree, the prediction value (just do the average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, x, y, idxs=None, min_leaf=5):\n",
    "        if idxs is None:\n",
    "            idxs = np.arange(len(y))\n",
    "        self.x, self.y, self.idxs, self.min_leaf = x, y, idxs, min_leaf\n",
    "        self.n, self.c = len(idxs), x.shape[1]\n",
    "        self.val = np.mean(y[idxs])\n",
    "        self.score = float('inf')\n",
    "        self.find_varsplit()\n",
    "        \n",
    "    def find_varsplit(self):\n",
    "        for i in range(self.c):\n",
    "            self.find_better_split(i)\n",
    "    \n",
    "    def find_better_split(self, var_idx):\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def split_name(self):\n",
    "        return self.x.columns[self.var_idx]\n",
    "    \n",
    "    @property\n",
    "    def split_col(self):\n",
    "        return self.x.values[self.idxs, self.val_idx]\n",
    "    \n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        return self.score == float('inf')\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = f'n: {self.n}; val:{self.val}'\n",
    "        if not self.is_leaf:\n",
    "            s += f'; score:{self.score}; split:{self.split}; var:{self.split_name}'\n",
    "        return s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
